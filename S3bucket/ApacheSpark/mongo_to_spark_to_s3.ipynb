{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import related pyspark library\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "from pyspark.sql.functions import from_json, col, struct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#import related data navigation / processing library\n",
    "import os\n",
    "\n",
    "# Load AWS credentials from environment variables\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "aws_region = os.getenv('AWS_DEFAULT_REGION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"S3ConnectionTest\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:x.x.x,\"\n",
    "            \"com.amazonaws:aws-java-sdk:x.x.x,\"\n",
    "            \"org.apache.spark:spark-sql-kafka:x.x.x,\"\n",
    "            \"org.mongodb.spark:mongo-spark-connector:x.x.x\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"enable region version here\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"enable region version here\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"fs.s3a.access.key\", aws_access_key_id) \\\n",
    "    .config(\"fs.s3a.secret.key\", aws_secret_access_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", f\"s3.{aws_region}.amazonaws.com\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch transaction data from mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data from mongodb and create dataframe\n",
    "# Read the data from MongoDB\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"spark.mongodb.input.uri\",\"input mongodb credential here\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattened the dataframe as One Wide Table Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the DataFrame\n",
    "# Optional: filter to certain nth row for testtin\n",
    "\n",
    "flattened_df = df.select(\n",
    "    col(\"trans_num\"),\n",
    "    col(\"trans_date_trans_time\"),\n",
    "    col(\"cc_num\"),\n",
    "    col(\"merchant\"),\n",
    "    col(\"category\"),\n",
    "    col(\"amt\"),\n",
    "    col(\"city_pop\"),\n",
    "    col(\"unix_time\"),\n",
    "    col(\"card_holder.first\").alias(\"card_holder_first\")\n",
    "    col(\"is_fraud\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert trans_date_trans_time to a date format\n",
    "flattened_df = flattened_df.withColumn(\n",
    "    'trans_date_trans_time',\n",
    "    from_unixtime(unix_timestamp('trans_date_trans_time', 'yyyy-MM-dd HH:mm:ss')).cast('timestamp')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to certain timeframe only (first two days for first run)\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Filter records for the date '2020-01-04' regardless of time\n",
    "filtered_df = flattened_df.filter(to_date(col('trans_date_trans_time')) <= '2020-01-02')\n",
    "\n",
    "# reinstate filtered value to flattened df\n",
    "flattened_df = filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding flattened json data to s3 bucket as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_s3(input_df):\n",
    "    # Define the output path\n",
    "    output_path = f\"s3a://your S3 bucket path\"\n",
    "\n",
    "    # Write the data to S3 in Parquet format\n",
    "    input_df.write \\\n",
    "        .partitionBy(\"your preferred partition style\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"compression\", \"gzip\") \\\n",
    "        .parquet(output_path)\n",
    "\n",
    "    print(f\"Data successfully written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data_to_s3(flattened_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
